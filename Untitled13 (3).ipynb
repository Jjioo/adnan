{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QeglH9dmij1h"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUKpwnOvT0WC",
        "outputId": "8b8beba7-2c15-40fa-8319-4edd50bc512b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_tR-73nxeQC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Load the CSV dataset and select only 'poem_text' and 'poem_title' columns\n",
        "file_path = '/content/drive/My Drive/all_poems.csv'\n",
        "df = pd.read_csv(file_path, usecols=['poem_text', 'poem_title'])\n",
        "\n",
        "df = df.sample(random_state=42)\n",
        "\n",
        "# Filter out rows with missing or empty poem_text values\n",
        "df = df.dropna(subset=['poem_text'])\n",
        "\n",
        "# Extract the poem text and title columns\n",
        "poem_text = df['poem_text'].values\n",
        "\n",
        "# Initialize and fit a tokenizer on the poem_text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(poem_text)\n",
        "\n",
        "# Convert text to sequences for poem_text\n",
        "sequences_text = tokenizer.texts_to_sequences(poem_text)\n",
        "\n",
        "# Create input sequences and target sequences for poem_text using NumPy\n",
        "input_sequences_text = []\n",
        "target_sequences_text = []\n",
        "\n",
        "for sequence in sequences_text:\n",
        "    for i in range(1, len(sequence)):\n",
        "        input_seq = sequence[:i]\n",
        "        target_seq = sequence[i]\n",
        "        input_sequences_text.append(input_seq)\n",
        "        target_sequences_text.append(target_seq)\n",
        "\n",
        "# Pad sequences to a fixed length for poem_text\n",
        "max_sequence_length_text = 100  # You can adjust this value\n",
        "input_sequences_text = pad_sequences(input_sequences_text, maxlen=max_sequence_length_text, padding='pre')\n",
        "target_sequences_text = np.array(target_sequences_text)  # Convert to NumPy array\n",
        "\n",
        "# Define and compile the model for poem_text\n",
        "vocab_size_text = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 128\n",
        "\n",
        "model_text = Sequential([\n",
        "    Embedding(input_dim=vocab_size_text, output_dim=embedding_dim, input_length=max_sequence_length_text),\n",
        "    LSTM(128),\n",
        "    Dense(vocab_size_text, activation='softmax')\n",
        "])\n",
        "\n",
        "model_text.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train the model using the dataset\n",
        "batch_size = 64\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_sequences_text, target_sequences_text))\n",
        "dataset = dataset.shuffle(buffer_size=len(input_sequences_text)).batch(batch_size)\n",
        "model_text.fit(dataset, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf8hBzmykpPy",
        "outputId": "e457984d-0e3f-45b7-d215-feaf66136bcc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3263/3263 [==============================] - 236s 70ms/step - loss: 9.8908\n",
            "Epoch 2/10\n",
            "3263/3263 [==============================] - 69s 21ms/step - loss: 9.1974\n",
            "Epoch 3/10\n",
            "3263/3263 [==============================] - 57s 17ms/step - loss: 8.5242\n",
            "Epoch 4/10\n",
            "3263/3263 [==============================] - 50s 15ms/step - loss: 7.5589\n",
            "Epoch 5/10\n",
            "3263/3263 [==============================] - 46s 14ms/step - loss: 6.5070\n",
            "Epoch 6/10\n",
            "3263/3263 [==============================] - 47s 14ms/step - loss: 5.4958\n",
            "Epoch 7/10\n",
            "3263/3263 [==============================] - 47s 14ms/step - loss: 4.5510\n",
            "Epoch 8/10\n",
            "3263/3263 [==============================] - 45s 14ms/step - loss: 3.6848\n",
            "Epoch 9/10\n",
            "3263/3263 [==============================] - 42s 13ms/step - loss: 2.9218\n",
            "Epoch 10/10\n",
            "3263/3263 [==============================] - 43s 13ms/step - loss: 2.2837\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7aa27fec8310>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ... (previous code)\n",
        "\n",
        "# Initialize a dictionary to map word indices to words\n",
        "index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n",
        "\n",
        "# Generate poetry for poem_text\n",
        "seed_text = \"البركة\"  # Customize your seed text for poem_text\n",
        "generated_poem = seed_text\n",
        "\n",
        "# Define the number of words you want to generate in the poem\n",
        "num_words_to_generate = 20 # Adjust the number of words as needed\n",
        "\n",
        "# Generate the poem word by word\n",
        "for _ in range(num_words_to_generate):\n",
        "    # Tokenize the current generated text\n",
        "    seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "    # Pad the seed sequence to match the model's input length\n",
        "    padded_sequence = pad_sequences([seed_sequence], maxlen=max_sequence_length_text, padding='pre')\n",
        "\n",
        "    # Predict the probabilities of the next word using the model\n",
        "    predicted_word_probabilities = model_text.predict(padded_sequence, verbose=0)\n",
        "\n",
        "    # Sample the next word based on predicted probabilities\n",
        "    predicted_word_index = np.random.choice(len(predicted_word_probabilities[0]), p=predicted_word_probabilities[0])\n",
        "\n",
        "    # Convert the predicted word index back to a word\n",
        "    predicted_word = index_to_word.get(predicted_word_index, \"\")\n",
        "\n",
        "    # Append the predicted word to the generated poem\n",
        "    generated_poem += \" \" + predicted_word\n",
        "\n",
        "    # Update the seed text with the new word\n",
        "    seed_text += \" \" + predicted_word\n",
        "\n",
        "# Print the generated poem\n",
        "print(generated_poem)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw2oiWi1niS-",
        "outputId": "eb3fb7f9-e0c9-4d0e-ab76-7130521897f0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "البركة في قلبي فك عينك لا انس كل احد الا اوفي من مختفيا فاذا ان لم تكن تجلدي يصلي ورافقني جسدي\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EIey7bc5w0s2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}